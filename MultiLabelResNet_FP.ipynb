{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b65b41-8cb1-449d-89de-6b5bbb89dd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import CelebA\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f7e079b-e924-4cc6-94de-f54395dd380e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "])\n",
    "\n",
    "# Load the CelebA dataset\n",
    "train_dataset = CelebA(root='data', split='train', target_type='attr', download=False, transform=transform)\n",
    "val_dataset = CelebA(root='data', split='valid', target_type='attr', download=False, transform=transform)\n",
    "test_dataset = CelebA(root='data', split='test', target_type='attr', download=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c489e237-02d9-43a9-8ce1-626172db56e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wgp3aq/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wgp3aq/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class MultiLabelResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelResNet, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(self.model.fc.in_features, 40),\n",
    "            nn.Sigmoid()  # Sigmoid activation for multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = MultiLabelResNet().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3580d37-d4ae-4073-b5bb-d432877b4650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch [1/10], Batch [100/2544], Loss: 0.2475\n",
      "Epoch [1/10], Batch [200/2544], Loss: 0.2497\n",
      "Epoch [1/10], Batch [300/2544], Loss: 0.2252\n",
      "Epoch [1/10], Batch [400/2544], Loss: 0.2395\n",
      "Epoch [1/10], Batch [500/2544], Loss: 0.2283\n",
      "Epoch [1/10], Batch [600/2544], Loss: 0.2414\n",
      "Epoch [1/10], Batch [700/2544], Loss: 0.2217\n",
      "Epoch [1/10], Batch [800/2544], Loss: 0.2240\n",
      "Epoch [1/10], Batch [900/2544], Loss: 0.2064\n",
      "Epoch [1/10], Batch [1000/2544], Loss: 0.2075\n",
      "Epoch [1/10], Batch [1100/2544], Loss: 0.1963\n",
      "Epoch [1/10], Batch [1200/2544], Loss: 0.2152\n",
      "Epoch [1/10], Batch [1300/2544], Loss: 0.2081\n",
      "Epoch [1/10], Batch [1400/2544], Loss: 0.2310\n",
      "Epoch [1/10], Batch [1500/2544], Loss: 0.2085\n",
      "Epoch [1/10], Batch [1600/2544], Loss: 0.2213\n",
      "Epoch [1/10], Batch [1700/2544], Loss: 0.2156\n",
      "Epoch [1/10], Batch [1800/2544], Loss: 0.2195\n",
      "Epoch [1/10], Batch [1900/2544], Loss: 0.2139\n",
      "Epoch [1/10], Batch [2000/2544], Loss: 0.2033\n",
      "Epoch [1/10], Batch [2100/2544], Loss: 0.2156\n",
      "Epoch [1/10], Batch [2200/2544], Loss: 0.2042\n",
      "Epoch [1/10], Batch [2300/2544], Loss: 0.1924\n",
      "Epoch [1/10], Batch [2400/2544], Loss: 0.1908\n",
      "Epoch [1/10], Batch [2500/2544], Loss: 0.2079\n",
      "Epoch [1/10], Average Training Loss: 0.2167\n",
      "Validation Loss: 0.2226, Validation Accuracy: 0.9013\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch [2/10], Batch [100/2544], Loss: 0.1988\n",
      "Epoch [2/10], Batch [200/2544], Loss: 0.1961\n",
      "Epoch [2/10], Batch [300/2544], Loss: 0.2090\n",
      "Epoch [2/10], Batch [400/2544], Loss: 0.2085\n",
      "Epoch [2/10], Batch [500/2544], Loss: 0.1956\n",
      "Epoch [2/10], Batch [600/2544], Loss: 0.1895\n",
      "Epoch [2/10], Batch [700/2544], Loss: 0.1906\n",
      "Epoch [2/10], Batch [800/2544], Loss: 0.1980\n",
      "Epoch [2/10], Batch [900/2544], Loss: 0.1957\n",
      "Epoch [2/10], Batch [1000/2544], Loss: 0.1966\n",
      "Epoch [2/10], Batch [1100/2544], Loss: 0.1958\n",
      "Epoch [2/10], Batch [1200/2544], Loss: 0.2050\n",
      "Epoch [2/10], Batch [1300/2544], Loss: 0.1788\n",
      "Epoch [2/10], Batch [1400/2544], Loss: 0.2093\n",
      "Epoch [2/10], Batch [1500/2544], Loss: 0.1909\n",
      "Epoch [2/10], Batch [1600/2544], Loss: 0.1834\n",
      "Epoch [2/10], Batch [1700/2544], Loss: 0.1889\n",
      "Epoch [2/10], Batch [1800/2544], Loss: 0.2034\n",
      "Epoch [2/10], Batch [1900/2544], Loss: 0.1888\n",
      "Epoch [2/10], Batch [2000/2544], Loss: 0.2093\n",
      "Epoch [2/10], Batch [2100/2544], Loss: 0.1938\n",
      "Epoch [2/10], Batch [2200/2544], Loss: 0.1968\n",
      "Epoch [2/10], Batch [2300/2544], Loss: 0.1995\n",
      "Epoch [2/10], Batch [2400/2544], Loss: 0.1901\n",
      "Epoch [2/10], Batch [2500/2544], Loss: 0.1936\n",
      "Epoch [2/10], Average Training Loss: 0.1951\n",
      "Validation Loss: 0.2136, Validation Accuracy: 0.9051\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch [3/10], Batch [100/2544], Loss: 0.1859\n",
      "Epoch [3/10], Batch [200/2544], Loss: 0.1844\n",
      "Epoch [3/10], Batch [300/2544], Loss: 0.1827\n",
      "Epoch [3/10], Batch [400/2544], Loss: 0.1962\n",
      "Epoch [3/10], Batch [500/2544], Loss: 0.1853\n",
      "Epoch [3/10], Batch [600/2544], Loss: 0.1627\n",
      "Epoch [3/10], Batch [700/2544], Loss: 0.1770\n",
      "Epoch [3/10], Batch [800/2544], Loss: 0.1780\n",
      "Epoch [3/10], Batch [900/2544], Loss: 0.1773\n",
      "Epoch [3/10], Batch [1000/2544], Loss: 0.1859\n",
      "Epoch [3/10], Batch [1100/2544], Loss: 0.1890\n",
      "Epoch [3/10], Batch [1200/2544], Loss: 0.1872\n",
      "Epoch [3/10], Batch [1300/2544], Loss: 0.1814\n",
      "Epoch [3/10], Batch [1400/2544], Loss: 0.1976\n",
      "Epoch [3/10], Batch [1500/2544], Loss: 0.1933\n",
      "Epoch [3/10], Batch [1600/2544], Loss: 0.1696\n",
      "Epoch [3/10], Batch [1700/2544], Loss: 0.1856\n",
      "Epoch [3/10], Batch [1800/2544], Loss: 0.1876\n",
      "Epoch [3/10], Batch [1900/2544], Loss: 0.1894\n",
      "Epoch [3/10], Batch [2000/2544], Loss: 0.1837\n",
      "Epoch [3/10], Batch [2100/2544], Loss: 0.1876\n",
      "Epoch [3/10], Batch [2200/2544], Loss: 0.1759\n",
      "Epoch [3/10], Batch [2300/2544], Loss: 0.1643\n",
      "Epoch [3/10], Batch [2400/2544], Loss: 0.1724\n",
      "Epoch [3/10], Batch [2500/2544], Loss: 0.1838\n",
      "Epoch [3/10], Average Training Loss: 0.1858\n",
      "Validation Loss: 0.1987, Validation Accuracy: 0.9138\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch [4/10], Batch [100/2544], Loss: 0.1782\n",
      "Epoch [4/10], Batch [200/2544], Loss: 0.1686\n",
      "Epoch [4/10], Batch [300/2544], Loss: 0.1854\n",
      "Epoch [4/10], Batch [400/2544], Loss: 0.1914\n",
      "Epoch [4/10], Batch [500/2544], Loss: 0.1765\n",
      "Epoch [4/10], Batch [600/2544], Loss: 0.1908\n",
      "Epoch [4/10], Batch [700/2544], Loss: 0.1795\n",
      "Epoch [4/10], Batch [800/2544], Loss: 0.1754\n",
      "Epoch [4/10], Batch [900/2544], Loss: 0.1754\n",
      "Epoch [4/10], Batch [1000/2544], Loss: 0.1693\n",
      "Epoch [4/10], Batch [1100/2544], Loss: 0.1894\n",
      "Epoch [4/10], Batch [1200/2544], Loss: 0.1736\n",
      "Epoch [4/10], Batch [1300/2544], Loss: 0.1878\n",
      "Epoch [4/10], Batch [1400/2544], Loss: 0.2017\n",
      "Epoch [4/10], Batch [1500/2544], Loss: 0.1887\n",
      "Epoch [4/10], Batch [1600/2544], Loss: 0.1681\n",
      "Epoch [4/10], Batch [1700/2544], Loss: 0.1847\n",
      "Epoch [4/10], Batch [1800/2544], Loss: 0.1705\n",
      "Epoch [4/10], Batch [1900/2544], Loss: 0.1836\n",
      "Epoch [4/10], Batch [2000/2544], Loss: 0.1633\n",
      "Epoch [4/10], Batch [2100/2544], Loss: 0.1975\n",
      "Epoch [4/10], Batch [2200/2544], Loss: 0.1652\n",
      "Epoch [4/10], Batch [2300/2544], Loss: 0.1917\n",
      "Epoch [4/10], Batch [2400/2544], Loss: 0.1856\n",
      "Epoch [4/10], Batch [2500/2544], Loss: 0.1788\n",
      "Epoch [4/10], Average Training Loss: 0.1774\n",
      "Validation Loss: 0.1924, Validation Accuracy: 0.9153\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch [5/10], Batch [100/2544], Loss: 0.1667\n",
      "Epoch [5/10], Batch [200/2544], Loss: 0.1530\n",
      "Epoch [5/10], Batch [300/2544], Loss: 0.1636\n",
      "Epoch [5/10], Batch [400/2544], Loss: 0.1621\n",
      "Epoch [5/10], Batch [500/2544], Loss: 0.1729\n",
      "Epoch [5/10], Batch [600/2544], Loss: 0.1623\n",
      "Epoch [5/10], Batch [700/2544], Loss: 0.1681\n",
      "Epoch [5/10], Batch [800/2544], Loss: 0.1690\n",
      "Epoch [5/10], Batch [900/2544], Loss: 0.1663\n",
      "Epoch [5/10], Batch [1000/2544], Loss: 0.1605\n",
      "Epoch [5/10], Batch [1100/2544], Loss: 0.1612\n",
      "Epoch [5/10], Batch [1200/2544], Loss: 0.1659\n",
      "Epoch [5/10], Batch [1300/2544], Loss: 0.1691\n",
      "Epoch [5/10], Batch [1400/2544], Loss: 0.1532\n",
      "Epoch [5/10], Batch [1500/2544], Loss: 0.1765\n",
      "Epoch [5/10], Batch [1600/2544], Loss: 0.1575\n",
      "Epoch [5/10], Batch [1700/2544], Loss: 0.1859\n",
      "Epoch [5/10], Batch [1800/2544], Loss: 0.1658\n",
      "Epoch [5/10], Batch [1900/2544], Loss: 0.1737\n",
      "Epoch [5/10], Batch [2000/2544], Loss: 0.1572\n",
      "Epoch [5/10], Batch [2100/2544], Loss: 0.1605\n",
      "Epoch [5/10], Batch [2200/2544], Loss: 0.1688\n",
      "Epoch [5/10], Batch [2300/2544], Loss: 0.1672\n",
      "Epoch [5/10], Batch [2400/2544], Loss: 0.1777\n",
      "Epoch [5/10], Batch [2500/2544], Loss: 0.1581\n",
      "Epoch [5/10], Average Training Loss: 0.1679\n",
      "Validation Loss: 0.2113, Validation Accuracy: 0.9117\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch [6/10], Batch [100/2544], Loss: 0.1821\n",
      "Epoch [6/10], Batch [200/2544], Loss: 0.1563\n",
      "Epoch [6/10], Batch [300/2544], Loss: 0.1494\n",
      "Epoch [6/10], Batch [400/2544], Loss: 0.1599\n",
      "Epoch [6/10], Batch [500/2544], Loss: 0.1537\n",
      "Epoch [6/10], Batch [600/2544], Loss: 0.1558\n",
      "Epoch [6/10], Batch [700/2544], Loss: 0.1753\n",
      "Epoch [6/10], Batch [800/2544], Loss: 0.1661\n",
      "Epoch [6/10], Batch [900/2544], Loss: 0.1640\n",
      "Epoch [6/10], Batch [1000/2544], Loss: 0.1562\n",
      "Epoch [6/10], Batch [1100/2544], Loss: 0.1583\n",
      "Epoch [6/10], Batch [1200/2544], Loss: 0.1470\n",
      "Epoch [6/10], Batch [1300/2544], Loss: 0.1493\n",
      "Epoch [6/10], Batch [1400/2544], Loss: 0.1538\n",
      "Epoch [6/10], Batch [1500/2544], Loss: 0.1604\n",
      "Epoch [6/10], Batch [1600/2544], Loss: 0.1513\n",
      "Epoch [6/10], Batch [1700/2544], Loss: 0.1634\n",
      "Epoch [6/10], Batch [1800/2544], Loss: 0.1768\n",
      "Epoch [6/10], Batch [1900/2544], Loss: 0.1771\n",
      "Epoch [6/10], Batch [2000/2544], Loss: 0.1613\n",
      "Epoch [6/10], Batch [2100/2544], Loss: 0.1536\n",
      "Epoch [6/10], Batch [2200/2544], Loss: 0.1574\n",
      "Epoch [6/10], Batch [2300/2544], Loss: 0.1589\n",
      "Epoch [6/10], Batch [2400/2544], Loss: 0.1566\n",
      "Epoch [6/10], Batch [2500/2544], Loss: 0.1492\n",
      "Epoch [6/10], Average Training Loss: 0.1567\n",
      "Validation Loss: 0.2561, Validation Accuracy: 0.8912\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch [7/10], Batch [100/2544], Loss: 0.1249\n",
      "Epoch [7/10], Batch [200/2544], Loss: 0.1355\n",
      "Epoch [7/10], Batch [300/2544], Loss: 0.1494\n",
      "Epoch [7/10], Batch [400/2544], Loss: 0.1555\n",
      "Epoch [7/10], Batch [500/2544], Loss: 0.1464\n",
      "Epoch [7/10], Batch [600/2544], Loss: 0.1532\n",
      "Epoch [7/10], Batch [700/2544], Loss: 0.1457\n",
      "Epoch [7/10], Batch [800/2544], Loss: 0.1515\n",
      "Epoch [7/10], Batch [900/2544], Loss: 0.1346\n",
      "Epoch [7/10], Batch [1000/2544], Loss: 0.1351\n",
      "Epoch [7/10], Batch [1100/2544], Loss: 0.1324\n",
      "Epoch [7/10], Batch [1200/2544], Loss: 0.1362\n",
      "Epoch [7/10], Batch [1300/2544], Loss: 0.1310\n",
      "Epoch [7/10], Batch [1400/2544], Loss: 0.1515\n",
      "Epoch [7/10], Batch [1500/2544], Loss: 0.1485\n",
      "Epoch [7/10], Batch [1600/2544], Loss: 0.1430\n",
      "Epoch [7/10], Batch [1700/2544], Loss: 0.1647\n",
      "Epoch [7/10], Batch [1800/2544], Loss: 0.1468\n",
      "Epoch [7/10], Batch [1900/2544], Loss: 0.1487\n",
      "Epoch [7/10], Batch [2000/2544], Loss: 0.1496\n",
      "Epoch [7/10], Batch [2100/2544], Loss: 0.1456\n",
      "Epoch [7/10], Batch [2200/2544], Loss: 0.1318\n",
      "Epoch [7/10], Batch [2300/2544], Loss: 0.1447\n",
      "Epoch [7/10], Batch [2400/2544], Loss: 0.1362\n",
      "Epoch [7/10], Batch [2500/2544], Loss: 0.1491\n",
      "Epoch [7/10], Average Training Loss: 0.1430\n",
      "Validation Loss: 0.2151, Validation Accuracy: 0.9111\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch [8/10], Batch [100/2544], Loss: 0.1157\n",
      "Epoch [8/10], Batch [200/2544], Loss: 0.1134\n",
      "Epoch [8/10], Batch [300/2544], Loss: 0.1282\n",
      "Epoch [8/10], Batch [400/2544], Loss: 0.1318\n",
      "Epoch [8/10], Batch [500/2544], Loss: 0.1209\n",
      "Epoch [8/10], Batch [600/2544], Loss: 0.1315\n",
      "Epoch [8/10], Batch [700/2544], Loss: 0.1209\n",
      "Epoch [8/10], Batch [800/2544], Loss: 0.1307\n",
      "Epoch [8/10], Batch [900/2544], Loss: 0.1192\n",
      "Epoch [8/10], Batch [1000/2544], Loss: 0.1293\n",
      "Epoch [8/10], Batch [1100/2544], Loss: 0.1085\n",
      "Epoch [8/10], Batch [1200/2544], Loss: 0.1262\n",
      "Epoch [8/10], Batch [1300/2544], Loss: 0.1259\n",
      "Epoch [8/10], Batch [1400/2544], Loss: 0.1310\n",
      "Epoch [8/10], Batch [1500/2544], Loss: 0.1260\n",
      "Epoch [8/10], Batch [1600/2544], Loss: 0.1391\n",
      "Epoch [8/10], Batch [1700/2544], Loss: 0.1303\n",
      "Epoch [8/10], Batch [1800/2544], Loss: 0.1234\n",
      "Epoch [8/10], Batch [1900/2544], Loss: 0.1566\n",
      "Epoch [8/10], Batch [2000/2544], Loss: 0.1242\n",
      "Epoch [8/10], Batch [2100/2544], Loss: 0.1424\n",
      "Epoch [8/10], Batch [2200/2544], Loss: 0.1280\n",
      "Epoch [8/10], Batch [2300/2544], Loss: 0.1341\n",
      "Epoch [8/10], Batch [2400/2544], Loss: 0.1398\n",
      "Epoch [8/10], Batch [2500/2544], Loss: 0.1280\n",
      "Epoch [8/10], Average Training Loss: 0.1274\n",
      "Validation Loss: 0.2243, Validation Accuracy: 0.9095\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch [9/10], Batch [100/2544], Loss: 0.1046\n",
      "Epoch [9/10], Batch [200/2544], Loss: 0.0967\n",
      "Epoch [9/10], Batch [300/2544], Loss: 0.1079\n",
      "Epoch [9/10], Batch [400/2544], Loss: 0.1131\n",
      "Epoch [9/10], Batch [500/2544], Loss: 0.1097\n",
      "Epoch [9/10], Batch [600/2544], Loss: 0.1250\n",
      "Epoch [9/10], Batch [700/2544], Loss: 0.1120\n",
      "Epoch [9/10], Batch [800/2544], Loss: 0.1003\n",
      "Epoch [9/10], Batch [900/2544], Loss: 0.1012\n",
      "Epoch [9/10], Batch [1000/2544], Loss: 0.1029\n",
      "Epoch [9/10], Batch [1100/2544], Loss: 0.1211\n",
      "Epoch [9/10], Batch [1200/2544], Loss: 0.1091\n",
      "Epoch [9/10], Batch [1300/2544], Loss: 0.1068\n",
      "Epoch [9/10], Batch [1400/2544], Loss: 0.1222\n",
      "Epoch [9/10], Batch [1500/2544], Loss: 0.1155\n",
      "Epoch [9/10], Batch [1600/2544], Loss: 0.1142\n",
      "Epoch [9/10], Batch [1700/2544], Loss: 0.1113\n",
      "Epoch [9/10], Batch [1800/2544], Loss: 0.1172\n",
      "Epoch [9/10], Batch [1900/2544], Loss: 0.1148\n",
      "Epoch [9/10], Batch [2000/2544], Loss: 0.1076\n",
      "Epoch [9/10], Batch [2100/2544], Loss: 0.1075\n",
      "Epoch [9/10], Batch [2200/2544], Loss: 0.1201\n",
      "Epoch [9/10], Batch [2300/2544], Loss: 0.1143\n",
      "Epoch [9/10], Batch [2400/2544], Loss: 0.1166\n",
      "Epoch [9/10], Batch [2500/2544], Loss: 0.1372\n",
      "Epoch [9/10], Average Training Loss: 0.1112\n",
      "Validation Loss: 0.2783, Validation Accuracy: 0.8945\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch [10/10], Batch [100/2544], Loss: 0.0868\n",
      "Epoch [10/10], Batch [200/2544], Loss: 0.0898\n",
      "Epoch [10/10], Batch [300/2544], Loss: 0.0901\n",
      "Epoch [10/10], Batch [400/2544], Loss: 0.1000\n",
      "Epoch [10/10], Batch [500/2544], Loss: 0.0853\n",
      "Epoch [10/10], Batch [600/2544], Loss: 0.0852\n",
      "Epoch [10/10], Batch [700/2544], Loss: 0.0875\n",
      "Epoch [10/10], Batch [1800/2544], Loss: 0.0999\n",
      "Epoch [10/10], Batch [1900/2544], Loss: 0.1015\n",
      "Epoch [10/10], Batch [2000/2544], Loss: 0.1028\n",
      "Epoch [10/10], Batch [2100/2544], Loss: 0.0995\n",
      "Epoch [10/10], Batch [2200/2544], Loss: 0.1107\n",
      "Epoch [10/10], Batch [2300/2544], Loss: 0.1013\n",
      "Epoch [10/10], Batch [2400/2544], Loss: 0.0975\n",
      "Epoch [10/10], Batch [2500/2544], Loss: 0.1023\n",
      "Epoch [10/10], Average Training Loss: 0.0958\n",
      "Validation Loss: 0.2704, Validation Accuracy: 0.9031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float()  # For BCELoss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]-------- Average Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            all_outputs.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Compute accuracy\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    preds = (all_outputs >= 0.5).float()  # Threshold at 0.5\n",
    "    correct = (preds == all_labels).float().mean().item()  # Mean accuracy over all attributes\n",
    "\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {correct:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20957da1-a9c7-4cc8-9d03-0156c4df4a85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Evaluation:\n",
      "Attribute 1 Accuracy: 0.9378\n",
      "Attribute 2 Accuracy: 0.8232\n",
      "Attribute 3 Accuracy: 0.8112\n",
      "Attribute 4 Accuracy: 0.8104\n",
      "Attribute 5 Accuracy: 0.9885\n",
      "Attribute 6 Accuracy: 0.9537\n",
      "Attribute 7 Accuracy: 0.6895\n",
      "Attribute 8 Accuracy: 0.8077\n",
      "Attribute 9 Accuracy: 0.8868\n",
      "Attribute 10 Accuracy: 0.9535\n",
      "Attribute 11 Accuracy: 0.9584\n",
      "Attribute 12 Accuracy: 0.8577\n",
      "Attribute 13 Accuracy: 0.8992\n",
      "Attribute 14 Accuracy: 0.9521\n",
      "Attribute 15 Accuracy: 0.9602\n",
      "Attribute 16 Accuracy: 0.9952\n",
      "Attribute 17 Accuracy: 0.9720\n",
      "Attribute 18 Accuracy: 0.9814\n",
      "Attribute 19 Accuracy: 0.9036\n",
      "Attribute 20 Accuracy: 0.8507\n",
      "Attribute 21 Accuracy: 0.9802\n",
      "Attribute 22 Accuracy: 0.9314\n",
      "Attribute 23 Accuracy: 0.9668\n",
      "Attribute 24 Accuracy: 0.8609\n",
      "Attribute 25 Accuracy: 0.9553\n",
      "Attribute 26 Accuracy: 0.7449\n",
      "Attribute 27 Accuracy: 0.9684\n",
      "Attribute 28 Accuracy: 0.7481\n",
      "Attribute 29 Accuracy: 0.9301\n",
      "Attribute 30 Accuracy: 0.9456\n",
      "Attribute 31 Accuracy: 0.9754\n",
      "Attribute 32 Accuracy: 0.9189\n",
      "Attribute 33 Accuracy: 0.8180\n",
      "Attribute 34 Accuracy: 0.8308\n",
      "Attribute 35 Accuracy: 0.8772\n",
      "Attribute 36 Accuracy: 0.9898\n",
      "Attribute 37 Accuracy: 0.9342\n",
      "Attribute 38 Accuracy: 0.8491\n",
      "Attribute 39 Accuracy: 0.9654\n",
      "Attribute 40 Accuracy: 0.8644\n",
      "Average Accuracy: 0.9012\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = model(images)\n",
    "            preds = (outputs > 0.5).float()  # Apply threshold\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    accuracies = []\n",
    "    for i in range(40):\n",
    "        acc = accuracy_score(all_labels[:, i], all_preds[:, i])\n",
    "        accuracies.append(acc)\n",
    "        print(f'Attribute {i+1} Accuracy: {acc:.4f}')\n",
    "    print(f'Average Accuracy: {np.mean(accuracies):.4f}')\n",
    "\n",
    "# Evaluate on test set\n",
    "\n",
    "print(\"Test Set Evaluation:\")\n",
    "evaluate(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
